Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='StanfordDog', dataset_dir='../Stanford_dog/For_FewShot', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> no checkpoint found at './DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar'
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(0): [100/1000]	Time 0.184 (0.212)	Loss 1.609 (1.609)	Prec@1 30.667 (25.254)
Test-(0): [200/1000]	Time 0.185 (0.213)	Loss 1.609 (1.609)	Prec@1 26.667 (25.499)
Test-(0): [300/1000]	Time 0.461 (0.212)	Loss 1.610 (1.609)	Prec@1 10.667 (25.302)
Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='StanfordDog', dataset_dir='../Stanford_dog/For_FewShot', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> loaded checkpoint './results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar' (epoch 3)
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(3): [100/1000]	Time 0.177 (0.209)	Loss 1.781 (1.206)	Prec@1 60.000 (53.162)
Test-(3): [200/1000]	Time 0.140 (0.198)	Loss 1.313 (1.195)	Prec@1 36.000 (53.446)
Test-(3): [300/1000]	Time 0.158 (0.197)	Loss 1.291 (1.205)	Prec@1 44.000 (52.757)
Test-(3): [400/1000]	Time 0.183 (0.194)	Loss 1.128 (1.203)	Prec@1 61.333 (52.914)
Test-(3): [500/1000]	Time 0.195 (0.195)	Loss 0.899 (1.201)	Prec@1 64.000 (52.894)
Test-(3): [600/1000]	Time 0.200 (0.194)	Loss 1.953 (1.199)	Prec@1 37.333 (52.976)
Test-(3): [700/1000]	Time 0.235 (0.195)	Loss 1.570 (1.203)	Prec@1 44.000 (52.793)
Test-(3): [800/1000]	Time 0.151 (0.196)	Loss 1.177 (1.204)	Prec@1 53.333 (52.856)
Test-(3): [900/1000]	Time 0.206 (0.194)	Loss 1.862 (1.200)	Prec@1 29.333 (52.898)
 * Prec@1 52.729 Best_prec1 48.705
Test accuracy 52.729336 h 0.6878487
===================================== Round 1 =====================================
Testset: 1000-------------1
Test-(3): [100/1000]	Time 0.191 (0.202)	Loss 1.413 (1.167)	Prec@1 45.333 (54.601)
Test-(3): [200/1000]	Time 0.161 (0.192)	Loss 1.441 (1.184)	Prec@1 50.667 (53.605)
Test-(3): [300/1000]	Time 0.178 (0.188)	Loss 1.253 (1.195)	Prec@1 46.667 (53.054)
Test-(3): [400/1000]	Time 0.174 (0.187)	Loss 1.203 (1.218)	Prec@1 62.667 (52.319)
Test-(3): [500/1000]	Time 0.144 (0.187)	Loss 1.214 (1.202)	Prec@1 56.000 (52.985)
Test-(3): [600/1000]	Time 0.187 (0.187)	Loss 1.207 (1.197)	Prec@1 58.667 (53.034)
Test-(3): [700/1000]	Time 0.185 (0.186)	Loss 1.181 (1.196)	Prec@1 52.000 (53.082)
Test-(3): [800/1000]	Time 0.213 (0.185)	Loss 1.708 (1.196)	Prec@1 32.000 (53.139)
Test-(3): [900/1000]	Time 0.212 (0.187)	Loss 1.624 (1.194)	Prec@1 33.333 (53.137)
 * Prec@1 53.105 Best_prec1 48.705
Test accuracy 53.105335 h 0.6810806
===================================== Round 2 =====================================
Testset: 1000-------------2
Test-(3): [100/1000]	Time 0.155 (0.217)	Loss 1.160 (1.165)	Prec@1 52.000 (54.535)
Test-(3): [200/1000]	Time 0.148 (0.201)	Loss 1.387 (1.170)	Prec@1 53.333 (54.554)
Test-(3): [300/1000]	Time 0.151 (0.196)	Loss 1.589 (1.191)	Prec@1 36.000 (53.493)
Test-(3): [400/1000]	Time 0.267 (0.196)	Loss 0.922 (1.197)	Prec@1 62.667 (53.077)
Test-(3): [500/1000]	Time 0.203 (0.196)	Loss 1.228 (1.194)	Prec@1 49.333 (53.168)
Test-(3): [600/1000]	Time 0.209 (0.195)	Loss 1.563 (1.198)	Prec@1 41.333 (52.930)
Test-(3): [700/1000]	Time 0.185 (0.195)	Loss 0.697 (1.201)	Prec@1 69.333 (52.727)
Test-(3): [800/1000]	Time 0.222 (0.195)	Loss 1.687 (1.203)	Prec@1 42.667 (52.616)
Test-(3): [900/1000]	Time 0.201 (0.194)	Loss 0.642 (1.202)	Prec@1 78.667 (52.716)
 * Prec@1 52.875 Best_prec1 48.705
Test accuracy 52.87467 h 0.674803
===================================== Round 3 =====================================
Testset: 1000-------------3
Test-(3): [100/1000]	Time 0.142 (0.230)	Loss 1.069 (1.170)	Prec@1 60.000 (53.030)
Test-(3): [200/1000]	Time 0.220 (0.215)	Loss 1.035 (1.203)	Prec@1 56.000 (52.398)
Test-(3): [300/1000]	Time 0.151 (0.211)	Loss 0.990 (1.173)	Prec@1 65.333 (53.927)
Test-(3): [400/1000]	Time 0.234 (0.205)	Loss 0.870 (1.176)	Prec@1 70.667 (53.746)
Test-(3): [500/1000]	Time 0.259 (0.203)	Loss 1.145 (1.176)	Prec@1 57.333 (53.682)
Test-(3): [600/1000]	Time 0.171 (0.203)	Loss 1.353 (1.177)	Prec@1 56.000 (53.728)
Test-(3): [700/1000]	Time 0.183 (0.203)	Loss 1.110 (1.181)	Prec@1 50.667 (53.560)
Test-(3): [800/1000]	Time 0.185 (0.202)	Loss 1.128 (1.182)	Prec@1 60.000 (53.503)
Test-(3): [900/1000]	Time 0.147 (0.200)	Loss 1.243 (1.181)	Prec@1 50.667 (53.680)
 * Prec@1 53.601 Best_prec1 48.705
Test accuracy 53.601334 h 0.69726264
===================================== Round 4 =====================================
Testset: 1000-------------4
Test-(3): [100/1000]	Time 0.192 (0.208)	Loss 1.292 (1.162)	Prec@1 41.333 (54.125)
Test-(3): [200/1000]	Time 0.203 (0.204)	Loss 0.855 (1.202)	Prec@1 65.333 (52.630)
Test-(3): [300/1000]	Time 0.241 (0.204)	Loss 1.089 (1.195)	Prec@1 56.000 (52.735)
Test-(3): [400/1000]	Time 0.166 (0.202)	Loss 0.931 (1.196)	Prec@1 62.667 (52.934)
Test-(3): [500/1000]	Time 0.237 (0.202)	Loss 1.177 (1.204)	Prec@1 50.667 (52.546)
Test-(3): [600/1000]	Time 0.158 (0.199)	Loss 1.080 (1.209)	Prec@1 57.333 (52.417)
Test-(3): [700/1000]	Time 0.164 (0.198)	Loss 1.105 (1.206)	Prec@1 53.333 (52.582)
Test-(3): [800/1000]	Time 0.154 (0.196)	Loss 0.794 (1.207)	Prec@1 68.000 (52.476)
Test-(3): [900/1000]	Time 0.199 (0.195)	Loss 1.434 (1.205)	Prec@1 45.333 (52.622)
 * Prec@1 52.547 Best_prec1 48.705
Test accuracy 52.546665 h 0.70342356
Aver_accuracy: 52.97147 Aver_h 0.6888836979866028
Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='StanfordDog', dataset_dir='../Stanford_dog/For_FewShot', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> loaded checkpoint './results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar' (epoch 3)
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(3): [100/1000]	Time 0.175 (0.214)	Loss 0.793 (1.232)	Prec@1 72.000 (51.749)
Test-(3): [200/1000]	Time 0.146 (0.199)	Loss 1.194 (1.210)	Prec@1 58.667 (52.949)
Test-(3): [300/1000]	Time 0.199 (0.194)	Loss 1.050 (1.191)	Prec@1 56.000 (53.546)
Test-(3): [400/1000]	Time 0.179 (0.191)	Loss 0.856 (1.192)	Prec@1 62.667 (53.393)
Test-(3): [500/1000]	Time 0.261 (0.191)	Loss 1.234 (1.202)	Prec@1 46.667 (53.054)
Test-(3): [600/1000]	Time 0.182 (0.191)	Loss 1.292 (1.194)	Prec@1 58.667 (53.369)
Test-(3): [700/1000]	Time 0.160 (0.192)	Loss 0.981 (1.193)	Prec@1 64.000 (53.166)
Test-(3): [800/1000]	Time 0.171 (0.192)	Loss 1.399 (1.194)	Prec@1 54.667 (53.192)
Test-(3): [900/1000]	Time 0.199 (0.192)	Loss 1.454 (1.197)	Prec@1 38.667 (53.171)
 * Prec@1 53.073 Best_prec1 48.705
Test accuracy 53.073338 h 0.6982662
===================================== Round 1 =====================================
Testset: 1000-------------1
Test-(3): [100/1000]	Time 0.184 (0.202)	Loss 1.059 (1.276)	Prec@1 54.667 (50.337)
Test-(3): [200/1000]	Time 0.167 (0.191)	Loss 1.962 (1.240)	Prec@1 50.667 (51.801)
Test-(3): [300/1000]	Time 0.146 (0.187)	Loss 0.554 (1.235)	Prec@1 82.667 (52.270)
Test-(3): [400/1000]	Time 0.160 (0.185)	Loss 1.251 (1.232)	Prec@1 41.333 (52.386)
Test-(3): [500/1000]	Time 0.235 (0.188)	Loss 1.288 (1.224)	Prec@1 46.667 (52.522)
Test-(3): [600/1000]	Time 0.258 (0.191)	Loss 1.177 (1.222)	Prec@1 49.333 (52.448)
Test-(3): [700/1000]	Time 0.211 (0.192)	Loss 0.566 (1.217)	Prec@1 82.667 (52.630)
Test-(3): [800/1000]	Time 0.191 (0.194)	Loss 1.102 (1.217)	Prec@1 60.000 (52.629)
Test-(3): [900/1000]	Time 0.213 (0.195)	Loss 0.827 (1.215)	Prec@1 68.000 (52.661)
 * Prec@1 52.832 Best_prec1 48.705
Test accuracy 52.832 h 0.68528765
===================================== Round 2 =====================================
Testset: 1000-------------2
Test-(3): [100/1000]	Time 0.241 (0.215)	Loss 0.834 (1.193)	Prec@1 64.000 (52.277)
Test-(3): [200/1000]	Time 0.285 (0.204)	Loss 1.226 (1.204)	Prec@1 45.333 (52.000)
Test-(3): [300/1000]	Time 0.251 (0.200)	Loss 1.338 (1.199)	Prec@1 45.333 (52.394)
Test-(3): [400/1000]	Time 0.158 (0.198)	Loss 1.276 (1.203)	Prec@1 54.667 (52.479)
Test-(3): [500/1000]	Time 0.192 (0.196)	Loss 1.293 (1.194)	Prec@1 60.000 (53.059)
Test-(3): [600/1000]	Time 0.246 (0.199)	Loss 0.753 (1.201)	Prec@1 80.000 (52.923)
Test-(3): [700/1000]	Time 0.222 (0.202)	Loss 0.942 (1.204)	Prec@1 61.333 (52.820)
Test-(3): [800/1000]	Time 0.208 (0.203)	Loss 1.259 (1.203)	Prec@1 46.667 (52.797)
Test-(3): [900/1000]	Time 0.156 (0.204)	Loss 1.255 (1.201)	Prec@1 52.000 (52.845)
 * Prec@1 52.832 Best_prec1 48.705
Test accuracy 52.832 h 0.7113031
===================================== Round 3 =====================================
Testset: 1000-------------3
Test-(3): [100/1000]	Time 0.196 (0.223)	Loss 1.111 (1.226)	Prec@1 54.667 (52.409)
Test-(3): [200/1000]	Time 0.190 (0.211)	Loss 1.184 (1.239)	Prec@1 52.000 (51.715)
Test-(3): [300/1000]	Time 0.183 (0.208)	Loss 1.157 (1.216)	Prec@1 48.000 (52.452)
Test-(3): [400/1000]	Time 0.187 (0.204)	Loss 1.344 (1.216)	Prec@1 42.667 (52.559)
Test-(3): [500/1000]	Time 0.192 (0.202)	Loss 1.124 (1.214)	Prec@1 53.333 (52.551)
Test-(3): [600/1000]	Time 0.160 (0.200)	Loss 1.072 (1.210)	Prec@1 54.667 (52.570)
Test-(3): [700/1000]	Time 0.162 (0.200)	Loss 1.344 (1.208)	Prec@1 46.667 (52.603)
Test-(3): [800/1000]	Time 0.178 (0.199)	Loss 1.373 (1.200)	Prec@1 45.333 (52.879)
Test-(3): [900/1000]	Time 0.164 (0.198)	Loss 0.924 (1.204)	Prec@1 58.667 (52.607)
 * Prec@1 52.649 Best_prec1 48.705
Test accuracy 52.649338 h 0.6654676
===================================== Round 4 =====================================
Testset: 1000-------------4
Test-(3): [100/1000]	Time 0.221 (0.220)	Loss 0.781 (1.197)	Prec@1 73.333 (53.162)
Test-(3): [200/1000]	Time 0.177 (0.206)	Loss 1.082 (1.188)	Prec@1 54.667 (52.823)
Test-(3): [300/1000]	Time 0.179 (0.201)	Loss 1.673 (1.202)	Prec@1 36.000 (52.580)
Test-(3): [400/1000]	Time 0.244 (0.202)	Loss 1.089 (1.198)	Prec@1 56.000 (52.785)
Test-(3): [500/1000]	Time 0.183 (0.204)	Loss 1.211 (1.197)	Prec@1 49.333 (52.745)
Test-(3): [600/1000]	Time 0.183 (0.203)	Loss 0.915 (1.201)	Prec@1 56.000 (52.559)
Test-(3): [700/1000]	Time 0.384 (0.204)	Loss 1.579 (1.201)	Prec@1 41.333 (52.567)
Test-(3): [800/1000]	Time 0.169 (0.206)	Loss 0.903 (1.200)	Prec@1 65.333 (52.643)
Test-(3): [900/1000]	Time 0.252 (0.207)	Loss 0.947 (1.199)	Prec@1 65.333 (52.832)
 * Prec@1 52.883 Best_prec1 48.705
Test accuracy 52.882668 h 0.69966215
Aver_accuracy: 52.85387 Aver_h 0.6919973492622375
Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='StanfordDog', dataset_dir='../Stanford_dog/For_FewShot', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> loaded checkpoint './results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar' (epoch 3)
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(3): [100/1000]	Time 0.168 (0.198)	Loss 1.234 (1.199)	Prec@1 33.333 (53.149)
Test-(3): [200/1000]	Time 0.139 (0.189)	Loss 1.182 (1.214)	Prec@1 52.000 (52.697)
Test-(3): [300/1000]	Time 0.146 (0.191)	Loss 1.106 (1.206)	Prec@1 54.667 (52.762)
Test-(3): [400/1000]	Time 0.188 (0.189)	Loss 1.160 (1.199)	Prec@1 46.667 (52.874)
Test-(3): [500/1000]	Time 0.142 (0.188)	Loss 1.171 (1.198)	Prec@1 45.333 (52.884)
Test-(3): [600/1000]	Time 0.179 (0.187)	Loss 0.896 (1.192)	Prec@1 62.667 (53.072)
Test-(3): [700/1000]	Time 0.210 (0.186)	Loss 0.987 (1.196)	Prec@1 62.667 (53.004)
Test-(3): [800/1000]	Time 0.187 (0.187)	Loss 1.056 (1.192)	Prec@1 58.667 (53.067)
Test-(3): [900/1000]	Time 0.184 (0.187)	Loss 1.468 (1.201)	Prec@1 34.667 (52.759)
 * Prec@1 52.681 Best_prec1 48.705
Test accuracy 52.681335 h 0.70796806
===================================== Round 1 =====================================
Testset: 1000-------------1
Test-(3): [100/1000]	Time 0.154 (0.205)	Loss 1.333 (1.190)	Prec@1 45.333 (53.901)
Test-(3): [200/1000]	Time 0.153 (0.193)	Loss 1.501 (1.185)	Prec@1 44.000 (53.393)
Test-(3): [300/1000]	Time 0.177 (0.191)	Loss 1.241 (1.179)	Prec@1 46.667 (53.502)
Test-(3): [400/1000]	Time 0.174 (0.190)	Loss 0.943 (1.193)	Prec@1 65.333 (53.234)
Test-(3): [500/1000]	Time 0.215 (0.189)	Loss 1.302 (1.205)	Prec@1 48.000 (52.822)
Test-(3): [600/1000]	Time 0.179 (0.188)	Loss 1.040 (1.209)	Prec@1 52.000 (52.703)
Test-(3): [700/1000]	Time 0.147 (0.188)	Loss 1.362 (1.201)	Prec@1 40.000 (52.825)
Test-(3): [800/1000]	Time 0.146 (0.188)	Loss 0.807 (1.201)	Prec@1 69.333 (52.796)
Test-(3): [900/1000]	Time 0.159 (0.188)	Loss 1.149 (1.198)	Prec@1 50.667 (52.907)
 * Prec@1 52.848 Best_prec1 48.705
Test accuracy 52.848 h 0.6688146
===================================== Round 2 =====================================
Testset: 1000-------------2
Test-(3): [100/1000]	Time 0.181 (0.202)	Loss 0.801 (1.188)	Prec@1 78.667 (53.531)
Test-(3): [200/1000]	Time 0.204 (0.191)	Loss 1.436 (1.224)	Prec@1 42.667 (52.332)
Test-(3): [300/1000]	Time 0.240 (0.189)	Loss 1.101 (1.219)	Prec@1 54.667 (52.474)
Test-(3): [400/1000]	Time 0.173 (0.187)	Loss 1.341 (1.229)	Prec@1 46.667 (52.067)
Test-(3): [500/1000]	Time 0.202 (0.186)	Loss 0.789 (1.224)	Prec@1 70.667 (52.295)
Test-(3): [600/1000]	Time 0.156 (0.186)	Loss 0.973 (1.222)	Prec@1 64.000 (52.306)
Test-(3): [700/1000]	Time 0.181 (0.186)	Loss 1.079 (1.220)	Prec@1 57.333 (52.352)
Test-(3): [800/1000]	Time 0.228 (0.187)	Loss 1.328 (1.223)	Prec@1 49.333 (52.311)
Test-(3): [900/1000]	Time 0.261 (0.187)	Loss 0.966 (1.225)	Prec@1 64.000 (52.247)
 * Prec@1 52.375 Best_prec1 48.705
Test accuracy 52.37467 h 0.67756146
===================================== Round 3 =====================================
Testset: 1000-------------3
Test-(3): [100/1000]	Time 0.182 (0.208)	Loss 1.617 (1.218)	Prec@1 30.667 (52.818)
Test-(3): [200/1000]	Time 0.183 (0.201)	Loss 1.110 (1.200)	Prec@1 53.333 (52.968)
Test-(3): [300/1000]	Time 0.144 (0.198)	Loss 0.942 (1.212)	Prec@1 64.000 (52.656)
Test-(3): [400/1000]	Time 0.189 (0.195)	Loss 1.403 (1.213)	Prec@1 46.667 (52.492)
Test-(3): [500/1000]	Time 0.209 (0.194)	Loss 1.100 (1.211)	Prec@1 49.333 (52.631)
Test-(3): [600/1000]	Time 0.190 (0.194)	Loss 1.702 (1.206)	Prec@1 25.333 (52.938)
Test-(3): [700/1000]	Time 0.197 (0.193)	Loss 1.301 (1.205)	Prec@1 52.000 (53.027)
Test-(3): [800/1000]	Time 0.160 (0.193)	Loss 0.960 (1.204)	Prec@1 68.000 (53.044)
Test-(3): [900/1000]	Time 0.183 (0.193)	Loss 1.170 (1.207)	Prec@1 52.000 (52.972)
 * Prec@1 53.032 Best_prec1 48.705
Test accuracy 53.032 h 0.66235614
===================================== Round 4 =====================================
Testset: 1000-------------4
Test-(3): [100/1000]	Time 0.204 (0.206)	Loss 0.835 (1.218)	Prec@1 66.667 (52.502)
Test-(3): [200/1000]	Time 0.155 (0.194)	Loss 1.792 (1.211)	Prec@1 45.333 (52.637)
Test-(3): [300/1000]	Time 0.228 (0.192)	Loss 1.400 (1.201)	Prec@1 42.667 (53.001)
Test-(3): [400/1000]	Time 0.146 (0.189)	Loss 1.247 (1.207)	Prec@1 54.667 (52.741)
Test-(3): [500/1000]	Time 0.161 (0.188)	Loss 1.209 (1.209)	Prec@1 46.667 (52.679)
Test-(3): [600/1000]	Time 0.151 (0.187)	Loss 1.248 (1.204)	Prec@1 53.333 (52.783)
Test-(3): [700/1000]	Time 0.174 (0.187)	Loss 2.447 (1.205)	Prec@1 40.000 (52.860)
Test-(3): [800/1000]	Time 0.182 (0.187)	Loss 1.147 (1.203)	Prec@1 53.333 (52.934)
Test-(3): [900/1000]	Time 0.186 (0.187)	Loss 1.196 (1.200)	Prec@1 48.000 (53.048)
 * Prec@1 53.104 Best_prec1 48.705
Test accuracy 53.104004 h 0.665172
Aver_accuracy: 52.808 Aver_h 0.6763744473457336
Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='StanfordDog', dataset_dir='../Stanford_dog/For_FewShot', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> loaded checkpoint './results/DM_SiameseNet_StanfordDog_Conv64F_5Way_1Shot_K3/model_best.pth.tar' (epoch 3)
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(3): [100/1000]	Time 0.181 (0.214)	Loss 1.215 (1.163)	Prec@1 54.667 (54.416)
Test-(3): [200/1000]	Time 0.212 (0.201)	Loss 1.474 (1.175)	Prec@1 32.000 (53.499)
Test-(3): [300/1000]	Time 0.198 (0.195)	Loss 1.242 (1.194)	Prec@1 54.667 (52.975)
Test-(3): [400/1000]	Time 0.186 (0.193)	Loss 1.218 (1.193)	Prec@1 52.000 (53.024)
Test-(3): [500/1000]	Time 0.189 (0.193)	Loss 1.028 (1.192)	Prec@1 50.667 (53.017)
Test-(3): [600/1000]	Time 0.228 (0.193)	Loss 0.772 (1.192)	Prec@1 73.333 (53.083)
Test-(3): [700/1000]	Time 0.214 (0.192)	Loss 1.155 (1.197)	Prec@1 56.000 (52.997)
Test-(3): [800/1000]	Time 0.190 (0.191)	Loss 1.168 (1.200)	Prec@1 44.000 (52.892)
Test-(3): [900/1000]	Time 0.210 (0.191)	Loss 1.125 (1.201)	Prec@1 53.333 (52.801)
 * Prec@1 52.777 Best_prec1 48.705
Test accuracy 52.777336 h 0.68968594
===================================== Round 1 =====================================
Testset: 1000-------------1
Test-(3): [100/1000]	Time 0.163 (0.212)	Loss 1.172 (1.204)	Prec@1 58.667 (52.211)
Test-(3): [200/1000]	Time 0.170 (0.206)	Loss 0.876 (1.190)	Prec@1 69.333 (52.816)
Test-(3): [300/1000]	Time 0.173 (0.207)	Loss 1.138 (1.215)	Prec@1 58.667 (52.102)
Test-(3): [400/1000]	Time 0.144 (0.207)	Loss 1.587 (1.214)	Prec@1 33.333 (52.170)
Test-(3): [500/1000]	Time 0.224 (0.205)	Loss 1.106 (1.213)	Prec@1 58.667 (52.402)
Test-(3): [600/1000]	Time 0.228 (0.205)	Loss 1.173 (1.213)	Prec@1 50.667 (52.315)
Test-(3): [700/1000]	Time 0.193 (0.204)	Loss 1.187 (1.207)	Prec@1 58.667 (52.548)
Test-(3): [800/1000]	Time 0.184 (0.203)	Loss 1.448 (1.206)	Prec@1 48.000 (52.418)
Test-(3): [900/1000]	Time 0.215 (0.202)	Loss 1.229 (1.207)	Prec@1 53.333 (52.457)
 * Prec@1 52.607 Best_prec1 48.705
Test accuracy 52.606663 h 0.7035798
===================================== Round 2 =====================================
Testset: 1000-------------2
Test-(3): [100/1000]	Time 0.159 (0.205)	Loss 1.133 (1.205)	Prec@1 50.667 (52.673)
Test-(3): [200/1000]	Time 0.150 (0.193)	Loss 1.374 (1.187)	Prec@1 34.667 (52.968)
Test-(3): [300/1000]	Time 0.205 (0.189)	Loss 1.391 (1.190)	Prec@1 37.333 (53.037)
Test-(3): [400/1000]	Time 0.183 (0.189)	Loss 1.220 (1.185)	Prec@1 48.000 (53.207)
Test-(3): [500/1000]	Time 0.149 (0.189)	Loss 0.865 (1.196)	Prec@1 64.000 (52.801)
Test-(3): [600/1000]	Time 0.153 (0.190)	Loss 0.962 (1.199)	Prec@1 64.000 (52.683)
Test-(3): [700/1000]	Time 0.192 (0.191)	Loss 0.725 (1.199)	Prec@1 64.000 (52.618)
Test-(3): [800/1000]	Time 0.158 (0.190)	Loss 1.313 (1.195)	Prec@1 45.333 (52.741)
Test-(3): [900/1000]	Time 0.184 (0.190)	Loss 1.575 (1.196)	Prec@1 38.667 (52.770)
 * Prec@1 52.723 Best_prec1 48.705
Test accuracy 52.722668 h 0.6764186
===================================== Round 3 =====================================
Testset: 1000-------------3
Test-(3): [100/1000]	Time 0.188 (0.212)	Loss 0.713 (1.172)	Prec@1 77.333 (54.799)
Test-(3): [200/1000]	Time 0.211 (0.202)	Loss 1.268 (1.180)	Prec@1 42.667 (54.348)
Test-(3): [300/1000]	Time 0.191 (0.199)	Loss 1.214 (1.188)	Prec@1 50.667 (53.834)
Test-(3): [400/1000]	Time 0.173 (0.198)	Loss 0.936 (1.194)	Prec@1 69.333 (53.629)
Test-(3): [500/1000]	Time 0.213 (0.197)	Loss 1.252 (1.194)	Prec@1 46.667 (53.352)
Test-(3): [600/1000]	Time 0.167 (0.196)	Loss 1.507 (1.204)	Prec@1 38.667 (52.998)
Test-(3): [700/1000]	Time 0.161 (0.196)	Loss 1.553 (1.206)	Prec@1 37.333 (52.938)
Test-(3): [800/1000]	Time 0.174 (0.195)	Loss 0.765 (1.208)	Prec@1 69.333 (52.782)
Test-(3): [900/1000]	Time 0.167 (0.195)	Loss 0.983 (1.209)	Prec@1 60.000 (52.744)
 * Prec@1 52.515 Best_prec1 48.705
Test accuracy 52.514668 h 0.6811975
===================================== Round 4 =====================================
Testset: 1000-------------4
Test-(3): [100/1000]	Time 0.161 (0.212)	Loss 1.126 (1.200)	Prec@1 52.000 (53.043)
Test-(3): [200/1000]	Time 0.163 (0.203)	Loss 1.249 (1.205)	Prec@1 49.333 (52.849)
Test-(3): [300/1000]	Time 0.186 (0.201)	Loss 1.070 (1.198)	Prec@1 56.000 (53.143)
Test-(3): [400/1000]	Time 0.186 (0.198)	Loss 1.674 (1.198)	Prec@1 37.333 (53.121)
Test-(3): [500/1000]	Time 0.161 (0.198)	Loss 1.421 (1.202)	Prec@1 52.000 (52.921)
Test-(3): [600/1000]	Time 0.194 (0.197)	Loss 0.823 (1.201)	Prec@1 74.667 (52.748)
Test-(3): [700/1000]	Time 0.164 (0.196)	Loss 1.178 (1.196)	Prec@1 57.333 (52.917)
Test-(3): [800/1000]	Time 0.201 (0.195)	Loss 1.460 (1.194)	Prec@1 40.000 (53.019)
Test-(3): [900/1000]	Time 0.214 (0.195)	Loss 1.333 (1.193)	Prec@1 49.333 (53.028)
 * Prec@1 53.001 Best_prec1 48.705
Test accuracy 53.00133 h 0.66838145
Aver_accuracy: 52.724537 Aver_h 0.6838526606559754
