Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='CubBird', dataset_dir='../CubBird', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> loaded checkpoint './results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3/model_best.pth.tar' (epoch 11)
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(11): [100/1000]	Time 0.174 (0.206)	Loss 0.737 (0.945)	Prec@1 68.000 (69.069)
Test-(11): [200/1000]	Time 0.190 (0.193)	Loss 1.595 (0.986)	Prec@1 56.000 (67.556)
Test-(11): [300/1000]	Time 0.191 (0.189)	Loss 1.326 (0.984)	Prec@1 66.667 (67.243)
Test-(11): [400/1000]	Time 0.165 (0.186)	Loss 0.945 (0.992)	Prec@1 77.333 (66.979)
Test-(11): [500/1000]	Time 0.203 (0.187)	Loss 0.657 (0.989)	Prec@1 74.667 (66.949)
Test-(11): [600/1000]	Time 0.187 (0.187)	Loss 0.470 (0.980)	Prec@1 88.000 (67.126)
Test-(11): [700/1000]	Time 0.199 (0.189)	Loss 0.942 (0.975)	Prec@1 69.333 (67.175)
Test-(11): [800/1000]	Time 0.188 (0.189)	Loss 1.735 (0.983)	Prec@1 45.333 (67.139)
Test-(11): [900/1000]	Time 0.166 (0.188)	Loss 0.579 (0.982)	Prec@1 80.000 (67.222)
 * Prec@1 67.087 Best_prec1 61.866
Test accuracy 67.08667 h 0.672209
===================================== Round 1 =====================================
Testset: 1000-------------1
Test-(11): [100/1000]	Time 0.194 (0.205)	Loss 0.765 (0.975)	Prec@1 80.000 (67.908)
Test-(11): [200/1000]	Time 0.171 (0.191)	Loss 1.132 (0.971)	Prec@1 56.000 (67.622)
Test-(11): [300/1000]	Time 0.179 (0.188)	Loss 1.030 (0.986)	Prec@1 62.667 (67.207)
Test-(11): [400/1000]	Time 0.190 (0.188)	Loss 0.703 (0.986)	Prec@1 73.333 (67.245)
Test-(11): [500/1000]	Time 0.182 (0.190)	Loss 0.912 (0.988)	Prec@1 61.333 (67.223)
Test-(11): [600/1000]	Time 0.177 (0.189)	Loss 1.575 (0.997)	Prec@1 48.000 (66.833)
Test-(11): [700/1000]	Time 0.189 (0.188)	Loss 1.227 (0.996)	Prec@1 54.667 (66.747)
Test-(11): [800/1000]	Time 0.162 (0.186)	Loss 1.024 (0.997)	Prec@1 66.667 (66.753)
Test-(11): [900/1000]	Time 0.171 (0.186)	Loss 0.593 (0.991)	Prec@1 76.000 (66.863)
 * Prec@1 66.972 Best_prec1 61.866
Test accuracy 66.972 h 0.6798963
===================================== Round 2 =====================================
Testset: 1000-------------2
Test-(11): [100/1000]	Time 0.168 (0.199)	Loss 1.227 (0.953)	Prec@1 60.000 (67.050)
Test-(11): [200/1000]	Time 0.175 (0.188)	Loss 0.991 (0.974)	Prec@1 61.333 (66.886)
Test-(11): [300/1000]	Time 0.179 (0.184)	Loss 0.424 (1.005)	Prec@1 86.667 (66.503)
Test-(11): [400/1000]	Time 0.181 (0.183)	Loss 1.560 (1.000)	Prec@1 48.000 (66.500)
Test-(11): [500/1000]	Time 0.175 (0.183)	Loss 1.319 (0.989)	Prec@1 56.000 (66.733)
Test-(11): [600/1000]	Time 0.190 (0.183)	Loss 0.869 (1.004)	Prec@1 72.000 (66.309)
Test-(11): [700/1000]	Time 0.179 (0.182)	Loss 0.799 (0.997)	Prec@1 72.000 (66.509)
Test-(11): [800/1000]	Time 0.197 (0.182)	Loss 0.528 (1.000)	Prec@1 80.000 (66.390)
Test-(11): [900/1000]	Time 0.169 (0.182)	Loss 1.151 (1.005)	Prec@1 65.333 (66.434)
 * Prec@1 66.545 Best_prec1 61.866
Test accuracy 66.545334 h 0.7003424
===================================== Round 3 =====================================
Testset: 1000-------------3
Test-(11): [100/1000]	Time 0.168 (0.205)	Loss 0.973 (0.995)	Prec@1 61.333 (65.822)
Test-(11): [200/1000]	Time 0.195 (0.192)	Loss 0.964 (0.991)	Prec@1 68.000 (66.441)
Test-(11): [300/1000]	Time 0.173 (0.188)	Loss 0.519 (0.993)	Prec@1 85.333 (66.374)
Test-(11): [400/1000]	Time 0.173 (0.186)	Loss 0.758 (0.989)	Prec@1 74.667 (66.760)
Test-(11): [500/1000]	Time 0.227 (0.186)	Loss 0.667 (0.986)	Prec@1 84.000 (66.861)
Test-(11): [600/1000]	Time 0.195 (0.185)	Loss 0.715 (0.993)	Prec@1 76.000 (66.869)
Test-(11): [700/1000]	Time 0.169 (0.185)	Loss 1.070 (0.995)	Prec@1 60.000 (66.937)
Test-(11): [800/1000]	Time 0.293 (0.185)	Loss 1.506 (0.992)	Prec@1 56.000 (66.990)
Test-(11): [900/1000]	Time 0.175 (0.185)	Loss 1.637 (0.991)	Prec@1 49.333 (67.037)
 * Prec@1 67.120 Best_prec1 61.866
Test accuracy 67.12 h 0.69009954
===================================== Round 4 =====================================
Testset: 1000-------------4
Test-(11): [100/1000]	Time 0.187 (0.199)	Loss 0.983 (1.008)	Prec@1 66.667 (65.611)
Test-(11): [200/1000]	Time 0.149 (0.189)	Loss 0.886 (0.953)	Prec@1 76.000 (67.801)
Test-(11): [300/1000]	Time 0.187 (0.187)	Loss 0.580 (0.963)	Prec@1 82.667 (67.646)
Test-(11): [400/1000]	Time 0.166 (0.185)	Loss 0.844 (0.966)	Prec@1 62.667 (67.508)
Test-(11): [500/1000]	Time 0.175 (0.184)	Loss 1.551 (0.968)	Prec@1 60.000 (67.646)
Test-(11): [600/1000]	Time 0.217 (0.184)	Loss 0.875 (0.980)	Prec@1 66.667 (67.525)
Test-(11): [700/1000]	Time 0.160 (0.185)	Loss 0.528 (0.972)	Prec@1 82.667 (67.726)
Test-(11): [800/1000]	Time 0.179 (0.185)	Loss 0.641 (0.971)	Prec@1 76.000 (67.717)
Test-(11): [900/1000]	Time 0.168 (0.184)	Loss 0.945 (0.967)	Prec@1 66.667 (67.882)
 * Prec@1 67.628 Best_prec1 61.866
Test accuracy 67.628 h 0.6767576
Aver_accuracy: 67.0704 Aver_h 0.683860969543457
Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='CubBird', dataset_dir='../CubBird', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> loaded checkpoint './results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3/model_best.pth.tar' (epoch 11)
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(11): [100/1000]	Time 0.189 (0.212)	Loss 0.961 (1.015)	Prec@1 66.667 (66.574)
Test-(11): [200/1000]	Time 0.128 (0.200)	Loss 1.717 (0.989)	Prec@1 48.000 (67.085)
Test-(11): [300/1000]	Time 0.169 (0.194)	Loss 0.831 (0.984)	Prec@1 60.000 (67.318)
Test-(11): [400/1000]	Time 0.178 (0.193)	Loss 0.728 (0.998)	Prec@1 77.333 (66.756)
Test-(11): [500/1000]	Time 0.167 (0.192)	Loss 0.407 (0.994)	Prec@1 82.667 (66.760)
Test-(11): [600/1000]	Time 0.153 (0.191)	Loss 1.531 (0.999)	Prec@1 66.667 (66.527)
Test-(11): [700/1000]	Time 0.175 (0.191)	Loss 0.683 (0.996)	Prec@1 77.333 (66.549)
Test-(11): [800/1000]	Time 0.203 (0.192)	Loss 0.778 (0.995)	Prec@1 70.667 (66.515)
Test-(11): [900/1000]	Time 0.186 (0.191)	Loss 2.033 (0.996)	Prec@1 44.000 (66.591)
 * Prec@1 66.385 Best_prec1 61.866
Test accuracy 66.38534 h 0.6842863
===================================== Round 1 =====================================
Testset: 1000-------------1
Test-(11): [100/1000]	Time 0.211 (0.210)	Loss 1.332 (1.010)	Prec@1 53.333 (66.125)
Test-(11): [200/1000]	Time 0.209 (0.198)	Loss 1.104 (0.981)	Prec@1 64.000 (66.667)
Test-(11): [300/1000]	Time 0.178 (0.197)	Loss 0.907 (0.990)	Prec@1 65.333 (66.937)
Test-(11): [400/1000]	Time 0.161 (0.192)	Loss 1.272 (0.982)	Prec@1 60.000 (67.219)
Test-(11): [500/1000]	Time 0.183 (0.190)	Loss 0.933 (0.979)	Prec@1 70.667 (67.260)
Test-(11): [600/1000]	Time 0.184 (0.188)	Loss 1.014 (0.978)	Prec@1 57.333 (67.206)
Test-(11): [700/1000]	Time 0.172 (0.191)	Loss 1.119 (0.976)	Prec@1 66.667 (67.222)
Test-(11): [800/1000]	Time 0.210 (0.191)	Loss 1.591 (0.981)	Prec@1 57.333 (67.166)
Test-(11): [900/1000]	Time 0.185 (0.190)	Loss 1.366 (0.981)	Prec@1 56.000 (67.191)
 * Prec@1 67.115 Best_prec1 61.866
Test accuracy 67.11466 h 0.6984729
===================================== Round 2 =====================================
Testset: 1000-------------2
Test-(11): [100/1000]	Time 0.157 (0.203)	Loss 1.609 (0.978)	Prec@1 42.667 (67.459)
Test-(11): [200/1000]	Time 0.167 (0.189)	Loss 1.139 (0.986)	Prec@1 61.333 (67.197)
Test-(11): [300/1000]	Time 0.166 (0.185)	Loss 0.984 (1.003)	Prec@1 57.333 (67.025)
Test-(11): [400/1000]	Time 0.192 (0.185)	Loss 0.448 (0.997)	Prec@1 82.667 (67.145)
Test-(11): [500/1000]	Time 0.183 (0.186)	Loss 1.259 (1.014)	Prec@1 56.000 (66.816)
Test-(11): [600/1000]	Time 0.198 (0.185)	Loss 0.839 (1.004)	Prec@1 69.333 (67.044)
Test-(11): [700/1000]	Time 0.186 (0.184)	Loss 1.302 (1.002)	Prec@1 61.333 (66.988)
Test-(11): [800/1000]	Time 0.177 (0.183)	Loss 1.575 (1.009)	Prec@1 41.333 (66.991)
Test-(11): [900/1000]	Time 0.172 (0.182)	Loss 0.987 (1.009)	Prec@1 58.667 (66.772)
 * Prec@1 66.708 Best_prec1 61.866
Test accuracy 66.708 h 0.6865037
===================================== Round 3 =====================================
Testset: 1000-------------3
Test-(11): [100/1000]	Time 0.182 (0.198)	Loss 0.322 (0.950)	Prec@1 88.000 (68.000)
Test-(11): [200/1000]	Time 0.172 (0.190)	Loss 1.503 (0.987)	Prec@1 46.667 (66.872)
Test-(11): [300/1000]	Time 0.210 (0.187)	Loss 0.840 (0.968)	Prec@1 68.000 (67.818)
Test-(11): [400/1000]	Time 0.178 (0.187)	Loss 0.403 (0.980)	Prec@1 84.000 (67.588)
Test-(11): [500/1000]	Time 0.188 (0.187)	Loss 0.527 (0.974)	Prec@1 82.667 (67.617)
Test-(11): [600/1000]	Time 0.209 (0.191)	Loss 1.448 (0.984)	Prec@1 65.333 (67.494)
Test-(11): [700/1000]	Time 0.169 (0.191)	Loss 1.161 (0.985)	Prec@1 58.667 (67.361)
Test-(11): [800/1000]	Time 0.174 (0.190)	Loss 1.625 (0.990)	Prec@1 52.000 (67.376)
Test-(11): [900/1000]	Time 0.187 (0.189)	Loss 1.074 (0.988)	Prec@1 61.333 (67.361)
 * Prec@1 67.119 Best_prec1 61.866
Test accuracy 67.118675 h 0.70217645
===================================== Round 4 =====================================
Testset: 1000-------------4
Test-(11): [100/1000]	Time 0.173 (0.205)	Loss 1.121 (1.007)	Prec@1 54.667 (66.403)
Test-(11): [200/1000]	Time 0.166 (0.191)	Loss 1.841 (1.001)	Prec@1 48.000 (66.521)
Test-(11): [300/1000]	Time 0.151 (0.187)	Loss 1.007 (0.991)	Prec@1 60.000 (66.848)
Test-(11): [400/1000]	Time 0.137 (0.186)	Loss 1.537 (0.991)	Prec@1 54.667 (66.936)
Test-(11): [500/1000]	Time 0.170 (0.185)	Loss 1.058 (1.010)	Prec@1 58.667 (66.422)
Test-(11): [600/1000]	Time 0.168 (0.184)	Loss 0.680 (1.010)	Prec@1 70.667 (66.511)
Test-(11): [700/1000]	Time 0.223 (0.183)	Loss 1.238 (1.001)	Prec@1 60.000 (66.659)
Test-(11): [800/1000]	Time 0.176 (0.184)	Loss 0.955 (0.995)	Prec@1 69.333 (66.788)
Test-(11): [900/1000]	Time 0.168 (0.184)	Loss 1.444 (0.991)	Prec@1 56.000 (66.887)
 * Prec@1 66.716 Best_prec1 61.866
Test accuracy 66.716 h 0.6994853
Aver_accuracy: 66.80854 Aver_h 0.694184935092926
Namespace(basemodel='Conv64F', beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, data_name='CubBird', dataset_dir='../CubBird', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.001, mode='test', nc=3, neighbor_k=3, ngpu=1, outf='./results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3', print_freq=100, query_num=15, resume='./results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3/model_best.pth.tar', shot_num=1, testepisodeSize=1, way_num=5, workers=8)
=> loaded checkpoint './results/DM_SiameseNet_CubBird_Conv64F_5Way_1Shot_K3/model_best.pth.tar' (epoch 11)
FourLayer_64F(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): DM_SiameseNet_Metric(
    (fully_connect1): Linear(in_features=28224, out_features=1, bias=True)
    (Norm_layer): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (FC_layer): Conv1d(1, 1, kernel_size=(2,), stride=(1,), dilation=(5,), bias=False)
  )
)
===================================== Round 0 =====================================
Testset: 1000-------------0
Test-(11): [100/1000]	Time 0.174 (0.214)	Loss 0.776 (0.977)	Prec@1 74.667 (66.733)
Test-(11): [200/1000]	Time 0.178 (0.197)	Loss 0.636 (0.962)	Prec@1 80.000 (67.144)
Test-(11): [300/1000]	Time 0.180 (0.191)	Loss 1.406 (0.984)	Prec@1 58.667 (67.167)
Test-(11): [400/1000]	Time 0.176 (0.191)	Loss 1.448 (0.982)	Prec@1 53.333 (67.215)
Test-(11): [500/1000]	Time 0.201 (0.189)	Loss 1.145 (0.993)	Prec@1 56.000 (66.845)
Test-(11): [600/1000]	Time 0.165 (0.188)	Loss 0.589 (0.998)	Prec@1 84.000 (66.871)
Test-(11): [700/1000]	Time 0.171 (0.186)	Loss 1.115 (0.991)	Prec@1 52.000 (66.975)
Test-(11): [800/1000]	Time 0.163 (0.186)	Loss 0.739 (0.990)	Prec@1 73.333 (66.920)
Test-(11): [900/1000]	Time 0.182 (0.185)	Loss 0.654 (0.991)	Prec@1 74.667 (66.908)
 * Prec@1 66.872 Best_prec1 61.866
Test accuracy 66.872 h 0.6789165
===================================== Round 1 =====================================
Testset: 1000-------------1
Test-(11): [100/1000]	Time 0.167 (0.203)	Loss 0.837 (0.999)	Prec@1 65.333 (66.917)
Test-(11): [200/1000]	Time 0.176 (0.189)	Loss 0.670 (0.998)	Prec@1 76.000 (67.390)
Test-(11): [300/1000]	Time 0.173 (0.186)	Loss 0.950 (0.994)	Prec@1 65.333 (67.393)
Test-(11): [400/1000]	Time 0.165 (0.183)	Loss 0.893 (0.998)	Prec@1 64.000 (67.126)
Test-(11): [500/1000]	Time 0.168 (0.182)	Loss 0.835 (0.998)	Prec@1 65.333 (67.066)
Test-(11): [600/1000]	Time 0.167 (0.181)	Loss 1.478 (1.008)	Prec@1 41.333 (66.738)
Test-(11): [700/1000]	Time 0.179 (0.181)	Loss 0.522 (1.005)	Prec@1 74.667 (66.728)
Test-(11): [800/1000]	Time 0.201 (0.181)	Loss 0.687 (0.997)	Prec@1 70.667 (66.965)
Test-(11): [900/1000]	Time 0.178 (0.181)	Loss 0.896 (0.991)	Prec@1 69.333 (67.136)
 * Prec@1 67.200 Best_prec1 61.866
Test accuracy 67.2 h 0.67504835
===================================== Round 2 =====================================
Testset: 1000-------------2
Test-(11): [100/1000]	Time 0.184 (0.201)	Loss 1.312 (0.969)	Prec@1 58.667 (67.564)
Test-(11): [200/1000]	Time 0.190 (0.193)	Loss 1.832 (1.025)	Prec@1 42.667 (65.917)
Test-(11): [300/1000]	Time 0.154 (0.191)	Loss 1.218 (1.008)	Prec@1 61.333 (66.534)
Test-(11): [400/1000]	Time 0.177 (0.188)	Loss 0.937 (1.020)	Prec@1 72.000 (66.138)
Test-(11): [500/1000]	Time 0.144 (0.186)	Loss 0.793 (1.001)	Prec@1 69.333 (66.651)
Test-(11): [600/1000]	Time 0.169 (0.185)	Loss 0.579 (1.007)	Prec@1 73.333 (66.647)
Test-(11): [700/1000]	Time 0.189 (0.184)	Loss 0.497 (1.006)	Prec@1 81.333 (66.748)
Test-(11): [800/1000]	Time 0.211 (0.185)	Loss 1.159 (1.010)	Prec@1 49.333 (66.632)
Test-(11): [900/1000]	Time 0.179 (0.185)	Loss 0.689 (1.011)	Prec@1 80.000 (66.427)
 * Prec@1 66.508 Best_prec1 61.866
Test accuracy 66.508 h 0.6911436
===================================== Round 3 =====================================
Testset: 1000-------------3
Test-(11): [100/1000]	Time 0.186 (0.211)	Loss 1.094 (1.043)	Prec@1 56.000 (65.373)
Test-(11): [200/1000]	Time 0.137 (0.196)	Loss 0.514 (1.036)	Prec@1 76.000 (65.990)
Test-(11): [300/1000]	Time 0.199 (0.193)	Loss 0.799 (1.022)	Prec@1 68.000 (66.308)
Test-(11): [400/1000]	Time 0.174 (0.190)	Loss 0.652 (1.005)	Prec@1 72.000 (66.470)
Test-(11): [500/1000]	Time 0.175 (0.188)	Loss 1.051 (1.017)	Prec@1 68.000 (66.377)
Test-(11): [600/1000]	Time 0.176 (0.189)	Loss 0.342 (1.023)	Prec@1 86.667 (66.179)
Test-(11): [700/1000]	Time 0.179 (0.189)	Loss 0.779 (1.012)	Prec@1 74.667 (66.425)
Test-(11): [800/1000]	Time 0.172 (0.187)	Loss 0.574 (1.009)	Prec@1 77.333 (66.620)
Test-(11): [900/1000]	Time 0.174 (0.187)	Loss 0.679 (1.008)	Prec@1 77.333 (66.643)
 * Prec@1 66.715 Best_prec1 61.866
Test accuracy 66.71467 h 0.6902599
===================================== Round 4 =====================================
Testset: 1000-------------4
Test-(11): [100/1000]	Time 0.203 (0.208)	Loss 0.619 (0.990)	Prec@1 77.333 (66.746)
Test-(11): [200/1000]	Time 0.171 (0.193)	Loss 1.109 (0.954)	Prec@1 60.000 (67.715)
Test-(11): [300/1000]	Time 0.177 (0.189)	Loss 0.682 (0.975)	Prec@1 74.667 (67.043)
Test-(11): [400/1000]	Time 0.171 (0.186)	Loss 1.463 (0.985)	Prec@1 49.333 (66.707)
Test-(11): [500/1000]	Time 0.183 (0.186)	Loss 1.702 (0.975)	Prec@1 32.000 (67.138)
Test-(11): [600/1000]	Time 0.213 (0.185)	Loss 0.910 (0.985)	Prec@1 64.000 (66.946)
Test-(11): [700/1000]	Time 0.160 (0.184)	Loss 0.702 (0.993)	Prec@1 77.333 (66.866)
Test-(11): [800/1000]	Time 0.171 (0.184)	Loss 0.832 (1.006)	Prec@1 69.333 (66.618)
Test-(11): [900/1000]	Time 0.188 (0.183)	Loss 0.817 (0.999)	Prec@1 68.000 (66.770)
 * Prec@1 67.005 Best_prec1 61.866
Test accuracy 67.00533 h 0.673165
Aver_accuracy: 66.86 Aver_h 0.6817066669464111
